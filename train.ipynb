{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split,RandomSampler, SequentialSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from apex import amp\n",
    "\n",
    "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
    "#https://github.com/TACJu/TransFG\n",
    "\n",
    "import timm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def reduce_mean(tensor, nprocs):\n",
    "    rt = tensor.clone()\n",
    "    #dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
    "    rt /= nprocs\n",
    "    return rt\n",
    "        \n",
    "def save_model(args, model):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(args.output_dir, \"%s_checkpoint.bin\" % args.name)\n",
    "    \n",
    "    if args.fp16:\n",
    "        checkpoint = {\n",
    "            'model': model_to_save.state_dict(),\n",
    "            'amp': amp.state_dict()\n",
    "        }\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            'model': model_to_save.state_dict(),\n",
    "        }\n",
    "\n",
    "    torch.save(checkpoint, model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", args.output_dir)\n",
    "    \n",
    "def setup(args):\n",
    "    # Prepare model \n",
    "    \n",
    "    num_classes = 200\n",
    "\n",
    "    model = timm.models.swin_large_patch4_window12_384_in22k(pretrained=True)\n",
    "    #a swin transformer model from timm\n",
    "    \n",
    "    model.head = torch.nn.Linear(in_features=1536, out_features=200, bias=True)\n",
    "    model.to(args.device)\n",
    "    num_params = count_parameters(model)\n",
    "    print(model)\n",
    "    \n",
    "    \n",
    "\n",
    "    logger.info(\"Training parameters %s\", args)\n",
    "    logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)\n",
    "    return args, model\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params/1000000\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "class birdDataset(Dataset):\n",
    "    def __init__(self, labels, imgs, transform):\n",
    "        self.transform = transform\n",
    "        self.lbls = labels\n",
    "        self.imgs = imgs\n",
    "        self.imgs = [f'dataset/train/{i}' for i in self.imgs]\n",
    "        assert len(self.imgs) == len(self.lbls), 'mismatched length!'\n",
    "        print('Total data in {}'.format(len(self.imgs)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpath =  self.imgs[index]\n",
    "        imgg = Image.open(imgpath).convert('RGB')\n",
    "        lbl = int(self.lbls[index])-1\n",
    "        imgg = self.transform(imgg)\n",
    "\n",
    "        return imgg, lbl\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "       \n",
    "def valid(args, model, writer, test_loader, global_step):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True,\n",
    "                          disable=False,\n",
    "                          position=0, \n",
    "                          leave=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_loss = eval_loss.mean()\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    accuracy = torch.tensor(accuracy).to(args.device)\n",
    "\n",
    "    val_accuracy = reduce_mean(accuracy, args.nprocs)\n",
    "    val_accuracy = val_accuracy.detach().cpu().numpy()\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % val_accuracy)\n",
    "\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "def train(args, model, train_loader, test_loader):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    \n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(\"logs\", args.name))\n",
    "    \n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=args.learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)   \n",
    "    t_total = args.num_steps\n",
    "    if args.decay_type == \"cosine\":\n",
    "        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    else:\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    \n",
    "    if args.fp16 :\n",
    "        model, optimizer = amp.initialize(models=model,\n",
    "                                          optimizers=optimizer,\n",
    "                                          opt_level=args.fp16_opt_level)\n",
    "        amp._amp_state.loss_scalers[0]._loss_scale = 2**20\n",
    "        args.fp16_isact = True\n",
    "    \n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Total optimization steps = %d\", args.num_steps)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Total train batch size = %d\",\n",
    "                args.train_batch_size)\n",
    "\n",
    "    model.zero_grad()\n",
    "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
    "    losses = AverageMeter()\n",
    "    global_step, best_acc = 0, 0\n",
    "    while True:\n",
    "        model.train()\n",
    "        epoch_iterator = tqdm(train_loader,\n",
    "                              desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                              bar_format=\"{l_bar}{r_bar}\",\n",
    "                              dynamic_ncols=True,\n",
    "                              disable=False)\n",
    "        all_preds, all_label = [], []\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            x, y = batch\n",
    "\n",
    "            #loss, logits = model(x, y)\n",
    "            logits = model(x)\n",
    "            loss = torch.nn.CrossEntropyLoss()(logits, y)\n",
    "            \n",
    "            loss = loss.mean()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            if len(all_preds) == 0:\n",
    "                all_preds.append(preds.detach().cpu().numpy())\n",
    "                all_label.append(y.detach().cpu().numpy())\n",
    "            else:\n",
    "                all_preds[0] = np.append(\n",
    "                    all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "                )\n",
    "                all_label[0] = np.append(\n",
    "                    all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "                )\n",
    "                \n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            losses.update(loss.item())\n",
    "\n",
    "            if args.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                \n",
    "            scheduler.step()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val)\n",
    "            )\n",
    "            \n",
    "            writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "            writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "            if global_step % args.eval_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    accuracy = valid(args, model, writer, test_loader, global_step)\n",
    "                \n",
    "                if best_acc < accuracy:\n",
    "                    save_model(args, model)\n",
    "                    best_acc = accuracy\n",
    "                logger.info(\"best accuracy so far: %f\" % best_acc)\n",
    "                model.train()\n",
    "\n",
    "            if global_step % t_total == 0:\n",
    "                break\n",
    "                \n",
    "        all_preds, all_label = all_preds[0], all_label[0]\n",
    "        accuracy = simple_accuracy(all_preds, all_label)\n",
    "        accuracy = torch.tensor(accuracy).to(args.device)\n",
    "        train_accuracy = reduce_mean(accuracy, args.nprocs)\n",
    "        train_accuracy = train_accuracy.detach().cpu().numpy()\n",
    "        logger.info(\"train accuracy so far: %f\" % train_accuracy)\n",
    "        \n",
    "        \n",
    "        losses.reset()\n",
    "        if global_step % t_total == 0:\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n",
    "    logger.info(\"End Training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/31/2021 18:35:58 - WARNING - __main__ - Process  device: cuda, n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Required parameters\n",
    "parser.add_argument(\"--name\", required=False, default = \"hw1\",\n",
    "                    help=\"Name of this run. Used for monitoring.\")\n",
    "parser.add_argument(\"--output_dir\", default=\"output/\", type=str,\n",
    "                    help=\"The output directory where checkpoints will be written.\")\n",
    "parser.add_argument(\"--train_batch_size\", default=16, type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=8, type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
    "                    help=\"Run prediction on validation set every so many steps.\"\n",
    "                         \"Will always run one evaluation at the end of training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
    "                    help=\"The initial learning rate for SGD.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
    "                    help=\"Weight deay if we apply some.\")\n",
    "parser.add_argument(\"--num_steps\", default=40000, type=int,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
    "                    help=\"How to decay the learning rate.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=100, type=int,\n",
    "                    help=\"Step of training to perform learning rate warmup for.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--fp16', action='store_true',default = True,\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument('--loss_scale', type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--smoothing_value', type=float, default=0.0,\n",
    "                    help=\"Label smoothing value\\n\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Setup CUDA, GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "args.device = device\n",
    "args.nprocs = torch.cuda.device_count()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if True else logging.WARN)\n",
    "logger.warning(\"Process  device: %s, n_gpu: %s\" % (args.device, args.n_gpu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/31/2021 18:36:01 - INFO - timm.models.helpers - Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)\n",
      "10/31/2021 18:36:03 - INFO - __main__ - Training parameters Namespace(decay_type='cosine', device=device(type='cuda'), eval_batch_size=8, eval_every=100, fp16=True, fp16_opt_level='O2', learning_rate=0.03, loss_scale=0, max_grad_norm=1.0, n_gpu=1, name='hw1', nprocs=1, num_steps=40000, output_dir='output/', seed=42, smoothing_value=0.0, train_batch_size=16, warmup_steps=100, weight_decay=0)\n",
      "10/31/2021 18:36:03 - INFO - __main__ - Total Parameter: \t195.5M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (0): BasicLayer(\n",
      "      dim=192, input_resolution=(96, 96), depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        input_resolution=(96, 96), dim=192\n",
      "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicLayer(\n",
      "      dim=384, input_resolution=(48, 48), depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        input_resolution=(48, 48), dim=384\n",
      "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicLayer(\n",
      "      dim=768, input_resolution=(24, 24), depth=18\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        input_resolution=(24, 24), dim=768\n",
      "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicLayer(\n",
      "      dim=1536, input_resolution=(12, 12), depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (head): Linear(in_features=1536, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Model & Tokenizer Setup\n",
    "args, model = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data in 2400\n",
      "Total data in 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\AppData\\Roaming\\Python\\Python37\\site-packages\\torchvision\\transforms\\transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "imgids = []\n",
    "labels = []\n",
    "with open('dataset/training_labels.txt',\"r\",encoding = \"utf-8\") as f:\n",
    "    for line in f.readlines():  \n",
    "        strr = line.split(\" \")\n",
    "        imgids.append(strr[0])\n",
    "        labels.append(strr[1][0:3])\n",
    "        \n",
    "#evenly distribute to training and testing 4:1\n",
    "training_imgids = []\n",
    "training_labels = []\n",
    "testing_imgids = []\n",
    "testing_labels = []\n",
    "number = np.zeros(200,dtype=int)\n",
    "for i in range(3000):\n",
    "    if number[int(labels[i])-1] < 12:\n",
    "        training_labels.append(labels[i])\n",
    "        training_imgids.append(imgids[i])\n",
    "        number[int(labels[i])-1] += 1\n",
    "    else:\n",
    "        testing_labels.append(labels[i])\n",
    "        testing_imgids.append(imgids[i])\n",
    "        number[int(labels[i])-1] += 1\n",
    "\n",
    "train_transform=transforms.Compose([transforms.Resize((420, 420), Image.BILINEAR), \n",
    "                                    #transforms.RandomResizedCrop(384),\n",
    "                                    transforms.RandomCrop((384, 384)),\n",
    "                                    #transforms.CenterCrop((384, 384)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomRotation(10),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "test_transform=transforms.Compose([transforms.Resize((420, 420), Image.BILINEAR),\n",
    "                                   transforms.CenterCrop((384, 384)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    \n",
    "trainset = birdDataset(training_labels, training_imgids, transform = train_transform)\n",
    "testset = birdDataset(testing_labels, testing_imgids, transform = test_transform)\n",
    "train_sampler = RandomSampler(trainset)\n",
    "test_sampler = SequentialSampler(testset)\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          sampler=train_sampler,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          #num_workers=0,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True)\n",
    "test_loader = DataLoader(testset,\n",
    "                         sampler=test_sampler,\n",
    "                         batch_size=args.eval_batch_size,\n",
    "                         #num_workers=0,\n",
    "                         pin_memory=True) if testset is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pertrain model\n",
    "pretrained_model = torch.load(\"output/hw1model/hw1model.bin\")['model']\n",
    "model.load_state_dict(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "10/31/2021 18:36:06 - INFO - __main__ - ***** Running training *****\n",
      "10/31/2021 18:36:06 - INFO - __main__ -   Total optimization steps = 10000\n",
      "10/31/2021 18:36:06 - INFO - __main__ -   Instantaneous batch size per GPU = 2\n",
      "10/31/2021 18:36:06 - INFO - __main__ -   Total train batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   0%|| 0/150 [00:00<?, ?it/s]C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Training (1 / 10000 Steps) (loss=0.00700):   0%|| 0/150 [00:02<?, ?it/s]C:\\Users\\YaShu\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Training (3 / 10000 Steps) (loss=0.02443):   2%|| 3/150 [00:07<06:25,  2.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b16ab1abab92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"freeze\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f3267f71e014>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, model, train_loader, test_loader)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 all_preds[0] = np.append(\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[0mall_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m                 )\n\u001b[0;32m    216\u001b[0m                 all_label[0] = np.append(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# freeze Training\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "for p in model.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in model.norm.parameters():\n",
    "    p.requires_grad = True\n",
    " \n",
    "for p, module in model.layers.named_children():    \n",
    "    if p == '3':\n",
    "        for r in module.parameters():\n",
    "            r.requires_grad = True\n",
    "    if p == '2':\n",
    "        for r in module.parameters():\n",
    "            r.requires_grad = False\n",
    "   \n",
    "      \n",
    "args.warmup_steps = 50\n",
    "args.train_batch_size = 2\n",
    "args.eval_batch_size = 2\n",
    "args.learning_rate = 0.01\n",
    "args.num_steps = 10000\n",
    "args.eval_every = int(2400/args.train_batch_size)\n",
    "args.weight_decay = 0.0\n",
    "args.name = \"freeze\"\n",
    "\n",
    "train(args, model, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
